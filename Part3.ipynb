{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219fe736-a1d2-480e-b095-1b28c0218912",
   "metadata": {},
   "source": [
    "Student Information\n",
    "- Name: 賴琮運\n",
    "- Student ID: 112034571\n",
    "- GitHub ID: ggbdmn\n",
    "- Kaggle name: 112034571\n",
    "- Kaggle private scoreboard snapshot: /img/pic0.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ae5cc-1a18-4646-9743-421c16551e42",
   "metadata": {},
   "source": [
    "Firstly, I tried to implement TFIDF on text and Random Forest as a baseline. Then, I looked deeper into the data and tried to preprocess it in a better way. Finally, I cleaned the text in several ways(mentioned in the code notebook), based on the data structure and characteristics. Also, I generated 4 types of features(W2V_on_text, TFIDF_on_hashtag, BERT_on_text, SentimentAnalyzer_on_text). I tried to utilize several models including RF, XGB, LSTM, and major voting through all, but LSTM lasted the best. Through the process, due to the large scale of the dataset, it is hard to precisely allocate the efficacy of RAM, CPU, GPU and even the cache storage, as I have no experience in doing this. However, after all did I find the way to activate my GPU and all other resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271261fd-7808-4c54-b61a-b683388dd35f",
   "metadata": {},
   "source": [
    "P.S. I do not think it is a good idea to append new description and new work to do especially 1 DAY before handing in the homework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
