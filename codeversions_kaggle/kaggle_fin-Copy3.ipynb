{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce48bf9b-b1ed-49e9-984c-3202ae81c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from contractions import fix\n",
    "import swifter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e28ae8-6bd0-4e6c-8c4b-826e2003b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('tweet/tweets_DM.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line)) \n",
    "f.close()\n",
    "\n",
    "emotion = pd.read_csv('tweet/emotion.csv')\n",
    "data_identification = pd.read_csv('tweet/data_identification.csv')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "_source = df['_source'].apply(lambda x: x['tweet'])\n",
    "df = pd.DataFrame({\n",
    "    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n",
    "    'hashtags': _source.apply(lambda x: x['hashtags']),\n",
    "    'text': _source.apply(lambda x: x['text']),\n",
    "})\n",
    "df = df.merge(data_identification, on='tweet_id', how='left')\n",
    "\n",
    "train_data = df[df['identification'] == 'train']\n",
    "test_data = df[df['identification'] == 'test']\n",
    "\n",
    "train_data = train_data.merge(emotion, on='tweet_id', how='left')\n",
    "train_data.drop_duplicates(subset=['text'], keep=False, inplace=True)\n",
    "\n",
    "train_data_sample = train_data.sample(frac=0.5, random_state=42)\n",
    "train_data_sample.to_pickle(\"train_dsample.pkl\")\n",
    "train_df = pd.read_pickle(\"train_dsample.pkl\")\n",
    "\n",
    "test_data.to_pickle(\"test_d.pkl\")\n",
    "test_df = pd.read_pickle(\"test_d.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f32ee6-46b1-4975-a8db-e3ff29e1b2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a43ea198e134fae858755f88d6ab9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/724591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d719615cd7af4e569b050040365e7ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/411972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x36fc6e</td>\n",
       "      <td>[development, future, winner]</td>\n",
       "      <td>Escaping pain is not the answer. Embracing pai...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>escaping pain is not_the answer embracing pain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x36f312</td>\n",
       "      <td>[ignored]</td>\n",
       "      <td>If I don't like you more then likely you've be...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>if i do not like you more then likely you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1d7398</td>\n",
       "      <td>[Silverdome]</td>\n",
       "      <td>Two stadiums I've been two and photographed, i...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>two stadium i have been two and photographed i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x26d0d1</td>\n",
       "      <td>[trans]</td>\n",
       "      <td>The racial trans badge  #trans &lt;LH&gt; &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>the racial trans badge trans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c580d</td>\n",
       "      <td>[weddingdressfitting]</td>\n",
       "      <td>Very special day with Luda and her mom ‚ù§Ô∏èFeeli...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>very special day with luda and her mom lovefee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724586</th>\n",
       "      <td>0x380c45</td>\n",
       "      <td>[Power5at5]</td>\n",
       "      <td>Hey @POWERATL @maddoxradio please play &lt;LH&gt; by...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>hey please play by on power5at5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724587</th>\n",
       "      <td>0x36c504</td>\n",
       "      <td>[]</td>\n",
       "      <td>damn my foot healed when everyone is already i...</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "      <td>damn my foot healed when everyone is already i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724588</th>\n",
       "      <td>0x2e8018</td>\n",
       "      <td>[ZENii, skincareroutine, moisturiser, health, ...</td>\n",
       "      <td>@skinandbodyclin we're excited üôå #ZENii #skinc...</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>were excited celebrate zenii skincareroutine m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724589</th>\n",
       "      <td>0x31e324</td>\n",
       "      <td>[job]</td>\n",
       "      <td>So excited! Just got a call that I have an int...</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "      <td>so excited ! just got a call that i have an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724590</th>\n",
       "      <td>0x23f502</td>\n",
       "      <td>[BB19Finale]</td>\n",
       "      <td>didn't know banging pots and pans all season c...</td>\n",
       "      <td>train</td>\n",
       "      <td>anger</td>\n",
       "      <td>did not know banging pot and pan all season co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>724591 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweet_id                                           hashtags  \\\n",
       "0       0x36fc6e                      [development, future, winner]   \n",
       "1       0x36f312                                          [ignored]   \n",
       "2       0x1d7398                                       [Silverdome]   \n",
       "3       0x26d0d1                                            [trans]   \n",
       "4       0x2c580d                              [weddingdressfitting]   \n",
       "...          ...                                                ...   \n",
       "724586  0x380c45                                        [Power5at5]   \n",
       "724587  0x36c504                                                 []   \n",
       "724588  0x2e8018  [ZENii, skincareroutine, moisturiser, health, ...   \n",
       "724589  0x31e324                                              [job]   \n",
       "724590  0x23f502                                       [BB19Finale]   \n",
       "\n",
       "                                                     text identification  \\\n",
       "0       Escaping pain is not the answer. Embracing pai...          train   \n",
       "1       If I don't like you more then likely you've be...          train   \n",
       "2       Two stadiums I've been two and photographed, i...          train   \n",
       "3                The racial trans badge  #trans <LH> <LH>          train   \n",
       "4       Very special day with Luda and her mom ‚ù§Ô∏èFeeli...          train   \n",
       "...                                                   ...            ...   \n",
       "724586  Hey @POWERATL @maddoxradio please play <LH> by...          train   \n",
       "724587  damn my foot healed when everyone is already i...          train   \n",
       "724588  @skinandbodyclin we're excited üôå #ZENii #skinc...          train   \n",
       "724589  So excited! Just got a call that I have an int...          train   \n",
       "724590  didn't know banging pots and pans all season c...          train   \n",
       "\n",
       "        emotion                                         clean_text  \n",
       "0       sadness  escaping pain is not_the answer embracing pain...  \n",
       "1       sadness  if i do not like you more then likely you have...  \n",
       "2       sadness  two stadium i have been two and photographed i...  \n",
       "3         trust                       the racial trans badge trans  \n",
       "4           joy  very special day with luda and her mom lovefee...  \n",
       "...         ...                                                ...  \n",
       "724586  sadness                    hey please play by on power5at5  \n",
       "724587  disgust  damn my foot healed when everyone is already i...  \n",
       "724588    trust  were excited celebrate zenii skincareroutine m...  \n",
       "724589     fear  so excited ! just got a call that i have an in...  \n",
       "724590    anger  did not know banging pot and pan all season co...  \n",
       "\n",
       "[724591 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import wordnet\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Ë°®ÊÉÖÁ¨¶ËôüÊõøÊèõË©ûÂÖ∏\n",
    "emoji_dict = {\n",
    "    'üòÇ': '[joy]', '‚ù§Ô∏è': '[love]', 'üòç': '[adoration]', 'üò≠': '[cry]',\n",
    "    '‚ù§': '[care]', 'üòä': '[happy]', 'üôè': '[pray]', 'üòò': '[kiss]',\n",
    "    'üíï': '[love_each_other]', 'üî•': '[fire]', 'üò©': '[weary]',\n",
    "    'ü§î': '[think]', 'üíØ': '[perfect]', 'üíô': '[loyalty]',\n",
    "    'üôÑ': '[annoyed]', 'üòÅ': '[happy]', 'üôå': '[celebrate]',\n",
    "    'üôèüèæ': '[pray]', 'üëç': '[approve]', 'üôèüèΩ': '[pray]'\n",
    "}\n",
    "\n",
    "# Define a dictionary for common Twitter abbreviations/slangs\n",
    "slang_dict = {\n",
    "    \"lol\": \"laugh out_loud\",\n",
    "    \"u\": \"you\",\n",
    "    \"idk\": \"I do not know\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"lmao\": \"laugh my_ass_off\",\n",
    "    \"lmfao\": \"laugh my_ass_off\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"brb\": \"be right back\"\n",
    "    # Add more as needed\n",
    "}\n",
    "\n",
    "# È†êËôïÁêÜÂáΩÊï∏\n",
    "def preprocess_text(text):\n",
    "    # ÊõøÊèõ emoji\n",
    "    for emj, keyword in emoji_dict.items():\n",
    "        text = text.replace(emj, keyword)\n",
    "    text = emoji.replace_emoji(text, replace='')  # ÁßªÈô§ÂÖ∂‰ªñ emoji\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # ÁßªÈô§Á∂≤ÂùÄ\n",
    "    text = re.sub(r'RT[\\s]+', '', text)  # Remove RT\n",
    "    text = text.replace('<LH>', '')\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)  # ÁßªÈô§ @user Âíå hashtags\n",
    "    #text = re.sub(r\"[^a-zA-Z0-9\\s]\", '', text)  # ÁßªÈô§ÁâπÊÆäÂ≠óÂÖÉ\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s!?]', '', text)\n",
    "    text = re.sub(r'not\\s+(\\w+)', r'not_\\1', text)\n",
    "    \n",
    "    wds = text.split()\n",
    "    tweet = \" \".join([slang_dict[wd.lower()] if wd.lower() in slang_dict else wd for wd in wds])\n",
    "    \n",
    "    text = fix(text)\n",
    "    #text = str(TextBlob(text).correct())\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = \" \".join([lemmatizer.lemmatize(wd) for wd in text.split()])\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([word for word in words])\n",
    "\n",
    "# Ê∏ÖÁêÜË®ìÁ∑¥ËàáÊ∏¨Ë©¶Ë≥áÊñô\n",
    "train_df['clean_text'] = train_df['text'].swifter.apply(preprocess_text)\n",
    "test_df['clean_text'] = test_df['text'].swifter.apply(preprocess_text)\n",
    "\n",
    "# Êâì‰∫ÇË®ìÁ∑¥Ë≥áÊñô\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a52a8f4-6970-435c-95e1-ec752f196567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5420989639f047aeb521c89c1879ceeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/724591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0c1d9c81eb408aa14e8702934ac26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/411972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#w2v preprocessing\n",
    "def w2v_preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([word for word in words if word not in stop_words])\n",
    "    \n",
    "train_df['w2v_text'] = train_df['clean_text'].swifter.apply(w2v_preprocess_text)\n",
    "test_df['w2v_text'] = test_df['clean_text'].swifter.apply(w2v_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef4052f9-9ef1-4419-a2aa-f6e7f757e432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>w2v_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x2de314</td>\n",
       "      <td>pharmtech ota</td>\n",
       "      <td>When your good at what you do offers come to y...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>when your good at what you do offer come to yo...</td>\n",
       "      <td>good offer come pharmtech ota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x264caf</td>\n",
       "      <td></td>\n",
       "      <td>Watching &lt;LH&gt; again after a really long time, ...</td>\n",
       "      <td>train</td>\n",
       "      <td>surprise</td>\n",
       "      <td>watching again after a really long time and de...</td>\n",
       "      <td>watching really long time dear god jack insuff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x27a5c3</td>\n",
       "      <td>Ido</td>\n",
       "      <td>I &lt;LH&gt; what I do #Ido what I love</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>i what i do ido what i love</td>\n",
       "      <td>ido love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x389940</td>\n",
       "      <td>depression</td>\n",
       "      <td>This is a new level of #depression, the old tr...</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>this is a new level of depression the old tric...</td>\n",
       "      <td>new level depression old trick used get around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a44ad</td>\n",
       "      <td>Memoir inspiration</td>\n",
       "      <td>@TwBookClub @GwenLeane A story of a man that w...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>a story of a man that will truly inspire every...</td>\n",
       "      <td>story man truly inspire everyone read memoir i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724586</th>\n",
       "      <td>0x28d4c9</td>\n",
       "      <td></td>\n",
       "      <td>Dear God, I'm afraid to die because I'm not fo...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>dear god i am afraid to die because i am not_f...</td>\n",
       "      <td>dear god afraid die not_following anymore plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724587</th>\n",
       "      <td>0x22ec3d</td>\n",
       "      <td>Life</td>\n",
       "      <td>70 The moments in your life are only once #Lif...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>70 the moment in your life are only once life ...</td>\n",
       "      <td>70 moment life life july 08 2017 0115am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724588</th>\n",
       "      <td>0x305abc</td>\n",
       "      <td>MukamaWanyi Delaware</td>\n",
       "      <td>Thanking &lt;LH&gt; for each &amp; everyday I wake up fr...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>thanking for each everyday i wake up fresh muk...</td>\n",
       "      <td>thanking everyday wake fresh mukamawanyi great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724589</th>\n",
       "      <td>0x2a947d</td>\n",
       "      <td></td>\n",
       "      <td>@realDonaldTrump That‚Äôs too bad that they chos...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>that is too bad that they chose to go</td>\n",
       "      <td>bad chose go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724590</th>\n",
       "      <td>0x365710</td>\n",
       "      <td>suave player ironing</td>\n",
       "      <td>Surprised my wife with a new ironing board tod...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>surprised my wife with a new ironing board tod...</td>\n",
       "      <td>surprised wife new ironing board today romance...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>724591 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweet_id              hashtags  \\\n",
       "0       0x2de314         pharmtech ota   \n",
       "1       0x264caf                         \n",
       "2       0x27a5c3                   Ido   \n",
       "3       0x389940            depression   \n",
       "4       0x2a44ad    Memoir inspiration   \n",
       "...          ...                   ...   \n",
       "724586  0x28d4c9                         \n",
       "724587  0x22ec3d                  Life   \n",
       "724588  0x305abc  MukamaWanyi Delaware   \n",
       "724589  0x2a947d                         \n",
       "724590  0x365710  suave player ironing   \n",
       "\n",
       "                                                     text identification  \\\n",
       "0       When your good at what you do offers come to y...          train   \n",
       "1       Watching <LH> again after a really long time, ...          train   \n",
       "2                       I <LH> what I do #Ido what I love          train   \n",
       "3       This is a new level of #depression, the old tr...          train   \n",
       "4       @TwBookClub @GwenLeane A story of a man that w...          train   \n",
       "...                                                   ...            ...   \n",
       "724586  Dear God, I'm afraid to die because I'm not fo...          train   \n",
       "724587  70 The moments in your life are only once #Lif...          train   \n",
       "724588  Thanking <LH> for each & everyday I wake up fr...          train   \n",
       "724589  @realDonaldTrump That‚Äôs too bad that they chos...          train   \n",
       "724590  Surprised my wife with a new ironing board tod...          train   \n",
       "\n",
       "             emotion                                         clean_text  \\\n",
       "0                joy  when your good at what you do offer come to yo...   \n",
       "1           surprise  watching again after a really long time and de...   \n",
       "2                joy                        i what i do ido what i love   \n",
       "3              trust  this is a new level of depression the old tric...   \n",
       "4       anticipation  a story of a man that will truly inspire every...   \n",
       "...              ...                                                ...   \n",
       "724586  anticipation  dear god i am afraid to die because i am not_f...   \n",
       "724587  anticipation  70 the moment in your life are only once life ...   \n",
       "724588  anticipation  thanking for each everyday i wake up fresh muk...   \n",
       "724589       sadness              that is too bad that they chose to go   \n",
       "724590           joy  surprised my wife with a new ironing board tod...   \n",
       "\n",
       "                                                 w2v_text  \n",
       "0                           good offer come pharmtech ota  \n",
       "1       watching really long time dear god jack insuff...  \n",
       "2                                                ido love  \n",
       "3       new level depression old trick used get around...  \n",
       "4       story man truly inspire everyone read memoir i...  \n",
       "...                                                   ...  \n",
       "724586  dear god afraid die not_following anymore plea...  \n",
       "724587            70 moment life life july 08 2017 0115am  \n",
       "724588  thanking everyday wake fresh mukamawanyi great...  \n",
       "724589                                       bad chose go  \n",
       "724590  surprised wife new ironing board today romance...  \n",
       "\n",
       "[724591 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "054ba0db-2d16-4490-9b8b-b471235cfcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Â∞áÊØèÊÆµÊñáÂ≠óËΩâÁÇ∫Ë©ûÂàóË°®\n",
    "train_sentences = train_df['w2v_text'].apply(lambda x: x.split()).tolist()\n",
    "test_sentences = test_df['w2v_text'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Ë®ìÁ∑¥ Word2Vec Ê®°Âûã\n",
    "w2v_model = Word2Vec(sentences=train_sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Â∞áÊñáÂ≠óËΩâÊèõÁÇ∫ÂêëÈáèÂπ≥ÂùáÂÄº\n",
    "def sentence_to_vector(sentence, model):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "train_text_vectors = np.array([sentence_to_vector(sent, w2v_model) for sent in train_sentences])\n",
    "test_text_vectors = np.array([sentence_to_vector(sent, w2v_model) for sent in test_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4435f750-620b-4b98-b304-809e3b4608b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Â∞á hashtags ÁµÑÂêàÊàêÂ≠ó‰∏≤\n",
    "train_df['hashtags'] = train_df['hashtags'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "test_df['hashtags'] = test_df['hashtags'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "\n",
    "# Ë®ìÁ∑¥ TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "tfidf_train = tfidf.fit_transform(train_df['hashtags'])\n",
    "tfidf_test = tfidf.transform(test_df['hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "803457d7-6355-4d26-b45b-a2705d7219bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = \"C:\\\\Users\\\\t1070\\\\.cache\\\\huggingface\\\\transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b694e7-cb5f-4c9f-8ce7-3e6c23640065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e9171216c54d09bc2d2f5fe771ff73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986473714c104fcb997284a89fc64b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\t1070\\.cache\\huggingface\\transformers\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196d29349fda44b099b9d55807e9289d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7dc450f81e47c681b3600849b445eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\t1070\\.cache\\huggingface\\transformers\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\t1070\\.cache\\huggingface\\transformers\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\t1070\\.cache\\huggingface\\transformers\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\t1070\\.cache\\huggingface\\transformers\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\t1070\\.cache\\huggingface\\transformers\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be88511ad114129892a7d1cf00aa2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at C:\\Users\\t1070\\.cache\\huggingface\\transformers\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\model.safetensors\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9cfc5e3db240ca88c2492fdd0af082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/724591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5efed4ae35f4de38106db01fefb2133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/411972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=cache_path)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, cache_dir=cache_path)\n",
    "\n",
    "# Â∞áÊ®°ÂûãÁßªËá≥ GPU\n",
    "model.to('cuda')\n",
    "\n",
    "def extract_bert_embeddings(text_series, batch_size=32):\n",
    "    def process_batch(batch_texts):\n",
    "        encodings = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        for key in encodings:\n",
    "            encodings[key] = encodings[key].to('cuda')  # ÁßªÂà∞ GPU\n",
    "        outputs = model(**encodings)\n",
    "        return outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()  # ÂõûÂà∞ CPU ÈÄ≤Ë°å NumPy ËôïÁêÜ\n",
    "\n",
    "    embeddings = (\n",
    "        text_series\n",
    "        .swifter.apply(lambda text: process_batch([text])[0])  # ÂñÆ‰∏ÄÊñáÊú¨ËôïÁêÜ\n",
    "        .to_numpy()\n",
    "    )\n",
    "    return np.stack(embeddings)\n",
    "\n",
    "bert_train = extract_bert_embeddings(train_df['clean_text'])\n",
    "bert_test = extract_bert_embeddings(test_df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7303df0-dc38-4cc1-961d-adc34d82ad5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d160f74155e4481a0741c02392eea10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/724591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c46e6139144390b7be52c92efa0f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/411972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# ‰ΩøÁî® NRC Lexicon ÁâπÂæµ\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "def extract_sentiment_features(text_series):\n",
    "    sentiment_scores = text_series.swifter.apply(lambda text: sid.polarity_scores(text))\n",
    "    return pd.DataFrame(list(sentiment_scores))\n",
    "\n",
    "sentiment_train = extract_sentiment_features(train_df['text'])\n",
    "sentiment_test = extract_sentiment_features(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e72fbbc3-4c11-4884-892c-e2ded24e592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# ËΩâÊèõÁÇ∫ GPU ÂºµÈáè‰∏¶Â†ÜÁñä\n",
    "train_features = torch.cat([\n",
    "    torch.tensor(tfidf_train.toarray(), dtype=torch.float32).to('cuda'),\n",
    "    torch.tensor(train_text_vectors, dtype=torch.float32).to('cuda'),\n",
    "    torch.tensor(bert_train, dtype=torch.float32).to('cuda'),\n",
    "    torch.tensor(sentiment_train.values.astype(np.float32), dtype=torch.float32).to('cuda')\n",
    "], dim=1)\n",
    "\n",
    "test_features = torch.cat([\n",
    "    torch.tensor(tfidf_test.toarray(), dtype=torch.float32).to('cuda'),\n",
    "    torch.tensor(test_text_vectors, dtype=torch.float32).to('cuda'),\n",
    "    torch.tensor(bert_test, dtype=torch.float32).to('cuda'),\n",
    "    torch.tensor(sentiment_test.values.astype(np.float32), dtype=torch.float32).to('cuda')\n",
    "], dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78178a4c-c983-4816-9dc5-1db7ad08c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ê®ôÁ±§Á∑®Á¢º\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df['emotion'])\n",
    "\n",
    "# Â∞áÁâπÂæµËàáÊ®ôÁ±§ÈÄ≤Ë°åÂàÜÂâ≤\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    train_features,  # Âæû train_features ‰∏≠ÂäÉÂàÜ\n",
    "    y_train, # Â∑≤Á∑®Á¢ºÁöÑÁõÆÊ®ôÂÄº\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train  # Á¢∫‰øùÈ°ûÂà•ÂàÜ‰Ωà‰∏ÄËá¥\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36c89680-f8b6-493e-8f0a-bddc34bbe47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DMLC_WORKER_TEMP'] = \"C:\\\\Users\\\\t1070\\\\.cache\\\\xgb\"\n",
    "\n",
    "# Ë®≠ÂÆöÁ∑©Â≠òË∑ØÂæë\n",
    "torch_cache_dir = \"C:\\\\Users\\\\t1070\\\\.cache\\\\torch\\\\cache\"\n",
    "os.environ['TORCH_HOME'] = torch_cache_dir  # ÊåáÂÆö PyTorch Ê®°ÂûãÂíåÊï∏ÊìöÁ∑©Â≠ò‰ΩçÁΩÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "802793fc-882d-4101-98c9-b123e98c569e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\t1070\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:01:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.00358\n",
      "[1]\tvalidation_0-mlogloss:1.94264\n",
      "[2]\tvalidation_0-mlogloss:1.89134\n",
      "[3]\tvalidation_0-mlogloss:1.84738\n",
      "[4]\tvalidation_0-mlogloss:1.80903\n",
      "[5]\tvalidation_0-mlogloss:1.77517\n",
      "[6]\tvalidation_0-mlogloss:1.74510\n",
      "[7]\tvalidation_0-mlogloss:1.71798\n",
      "[8]\tvalidation_0-mlogloss:1.69325\n",
      "[9]\tvalidation_0-mlogloss:1.67111\n",
      "[10]\tvalidation_0-mlogloss:1.65094\n",
      "[11]\tvalidation_0-mlogloss:1.63246\n",
      "[12]\tvalidation_0-mlogloss:1.61555\n",
      "[13]\tvalidation_0-mlogloss:1.60044\n",
      "[14]\tvalidation_0-mlogloss:1.58632\n",
      "[15]\tvalidation_0-mlogloss:1.57323\n",
      "[16]\tvalidation_0-mlogloss:1.56107\n",
      "[17]\tvalidation_0-mlogloss:1.54969\n",
      "[18]\tvalidation_0-mlogloss:1.53928\n",
      "[19]\tvalidation_0-mlogloss:1.52957\n",
      "[20]\tvalidation_0-mlogloss:1.52057\n",
      "[21]\tvalidation_0-mlogloss:1.51220\n",
      "[22]\tvalidation_0-mlogloss:1.50435\n",
      "[23]\tvalidation_0-mlogloss:1.49708\n",
      "[24]\tvalidation_0-mlogloss:1.49024\n",
      "[25]\tvalidation_0-mlogloss:1.48381\n",
      "[26]\tvalidation_0-mlogloss:1.47769\n",
      "[27]\tvalidation_0-mlogloss:1.47194\n",
      "[28]\tvalidation_0-mlogloss:1.46655\n",
      "[29]\tvalidation_0-mlogloss:1.46153\n",
      "[30]\tvalidation_0-mlogloss:1.45691\n",
      "[31]\tvalidation_0-mlogloss:1.45242\n",
      "[32]\tvalidation_0-mlogloss:1.44806\n",
      "[33]\tvalidation_0-mlogloss:1.44399\n",
      "[34]\tvalidation_0-mlogloss:1.44018\n",
      "[35]\tvalidation_0-mlogloss:1.43642\n",
      "[36]\tvalidation_0-mlogloss:1.43295\n",
      "[37]\tvalidation_0-mlogloss:1.42959\n",
      "[38]\tvalidation_0-mlogloss:1.42646\n",
      "[39]\tvalidation_0-mlogloss:1.42355\n",
      "[40]\tvalidation_0-mlogloss:1.42056\n",
      "[41]\tvalidation_0-mlogloss:1.41763\n",
      "[42]\tvalidation_0-mlogloss:1.41509\n",
      "[43]\tvalidation_0-mlogloss:1.41236\n",
      "[44]\tvalidation_0-mlogloss:1.41003\n",
      "[45]\tvalidation_0-mlogloss:1.40772\n",
      "[46]\tvalidation_0-mlogloss:1.40559\n",
      "[47]\tvalidation_0-mlogloss:1.40345\n",
      "[48]\tvalidation_0-mlogloss:1.40141\n",
      "[49]\tvalidation_0-mlogloss:1.39937\n",
      "[50]\tvalidation_0-mlogloss:1.39758\n",
      "[51]\tvalidation_0-mlogloss:1.39559\n",
      "[52]\tvalidation_0-mlogloss:1.39380\n",
      "[53]\tvalidation_0-mlogloss:1.39197\n",
      "[54]\tvalidation_0-mlogloss:1.39021\n",
      "[55]\tvalidation_0-mlogloss:1.38863\n",
      "[56]\tvalidation_0-mlogloss:1.38698\n",
      "[57]\tvalidation_0-mlogloss:1.38562\n",
      "[58]\tvalidation_0-mlogloss:1.38418\n",
      "[59]\tvalidation_0-mlogloss:1.38281\n",
      "[60]\tvalidation_0-mlogloss:1.38139\n",
      "[61]\tvalidation_0-mlogloss:1.37999\n",
      "[62]\tvalidation_0-mlogloss:1.37875\n",
      "[63]\tvalidation_0-mlogloss:1.37746\n",
      "[64]\tvalidation_0-mlogloss:1.37626\n",
      "[65]\tvalidation_0-mlogloss:1.37506\n",
      "[66]\tvalidation_0-mlogloss:1.37395\n",
      "[67]\tvalidation_0-mlogloss:1.37300\n",
      "[68]\tvalidation_0-mlogloss:1.37182\n",
      "[69]\tvalidation_0-mlogloss:1.37063\n",
      "[70]\tvalidation_0-mlogloss:1.36969\n",
      "[71]\tvalidation_0-mlogloss:1.36863\n",
      "[72]\tvalidation_0-mlogloss:1.36766\n",
      "[73]\tvalidation_0-mlogloss:1.36655\n",
      "[74]\tvalidation_0-mlogloss:1.36564\n",
      "[75]\tvalidation_0-mlogloss:1.36468\n",
      "[76]\tvalidation_0-mlogloss:1.36382\n",
      "[77]\tvalidation_0-mlogloss:1.36295\n",
      "[78]\tvalidation_0-mlogloss:1.36205\n",
      "[79]\tvalidation_0-mlogloss:1.36131\n",
      "[80]\tvalidation_0-mlogloss:1.36038\n",
      "[81]\tvalidation_0-mlogloss:1.35960\n",
      "[82]\tvalidation_0-mlogloss:1.35877\n",
      "[83]\tvalidation_0-mlogloss:1.35811\n",
      "[84]\tvalidation_0-mlogloss:1.35728\n",
      "[85]\tvalidation_0-mlogloss:1.35649\n",
      "[86]\tvalidation_0-mlogloss:1.35572\n",
      "[87]\tvalidation_0-mlogloss:1.35495\n",
      "[88]\tvalidation_0-mlogloss:1.35430\n",
      "[89]\tvalidation_0-mlogloss:1.35355\n",
      "[90]\tvalidation_0-mlogloss:1.35278\n",
      "[91]\tvalidation_0-mlogloss:1.35219\n",
      "[92]\tvalidation_0-mlogloss:1.35152\n",
      "[93]\tvalidation_0-mlogloss:1.35089\n",
      "[94]\tvalidation_0-mlogloss:1.35019\n",
      "[95]\tvalidation_0-mlogloss:1.34969\n",
      "[96]\tvalidation_0-mlogloss:1.34903\n",
      "[97]\tvalidation_0-mlogloss:1.34822\n",
      "[98]\tvalidation_0-mlogloss:1.34755\n",
      "[99]\tvalidation_0-mlogloss:1.34707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\t1070\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:17:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (GPU) Confusion Matrix:\n",
      "[[  494   161   679    32  1659   907     1    22]\n",
      " [   24 11262   824    67 10717  1430     3   420]\n",
      " [   77   412  4550   137  5189  3481     3    88]\n",
      " [    6   380   460  1330  3331   809     5    44]\n",
      " [   30  2753  1181   170 44296  2164     6   816]\n",
      " [  104   701  2523   127  8310  7382     9    93]\n",
      " [   10   183   553    36  2475   903   556    29]\n",
      " [   13  1643   664    42 13691   994     3  3455]]\n",
      "\n",
      "XGBoost (GPU) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.12      0.21      3955\n",
      "           1       0.64      0.46      0.53     24747\n",
      "           2       0.40      0.33      0.36     13937\n",
      "           3       0.69      0.21      0.32      6365\n",
      "           4       0.49      0.86      0.63     51416\n",
      "           5       0.41      0.38      0.40     19249\n",
      "           6       0.95      0.12      0.21      4745\n",
      "           7       0.70      0.17      0.27     20505\n",
      "\n",
      "    accuracy                           0.51    144919\n",
      "   macro avg       0.62      0.33      0.37    144919\n",
      "weighted avg       0.56      0.51      0.47    144919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBÊ®°ÂûãÂÆöÁæ©\n",
    "# 1. XGBoost\n",
    "# xgb_model = xgb.XGBClassifier(\n",
    "#     objective='multi:softmax', \n",
    "#     num_class=len(label_encoder.classes_), \n",
    "#     use_label_encoder=False, \n",
    "#     eval_metric='mlogloss', \n",
    "#     max_depth=6, \n",
    "#     learning_rate=0.01, \n",
    "#     n_estimators=500\n",
    "# )\n",
    "\n",
    "# # Ë®ìÁ∑¥ XGBoost\n",
    "# xgb_model.fit(X_train_split.cpu().numpy(), y_train_split)\n",
    "\n",
    "# # È©óË≠â XGBoost\n",
    "# xgb_val_preds = xgb_model.predict(X_val_split.cpu().numpy())\n",
    "# print(\"XGBoost Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_val_split, xgb_val_preds))\n",
    "# print(\"\\nXGBoost Classification Report:\")\n",
    "# print(classification_report(y_val_split, xgb_val_preds))\n",
    "\n",
    "# 1. XGBoost with GPU support\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax', \n",
    "    num_class=len(label_encoder.classes_), \n",
    "    use_label_encoder=False, \n",
    "    eval_metric='mlogloss', \n",
    "    max_depth=6, \n",
    "    learning_rate=0.1, \n",
    "    n_estimators=100,\n",
    "    tree_method = \"hist\", device = \"cuda\",\n",
    "    verbosity = 2\n",
    "    #predictor='gpu_predictor'  # ÈÅãË°åÊôÇ‰πü‰ΩøÁî® GPU\n",
    ")\n",
    "\n",
    "# Ë®ìÁ∑¥ XGBoost\n",
    "xgb_model.fit(\n",
    "    X_train_split.cpu().numpy(),\n",
    "    y_train_split,\n",
    "    eval_set=[(X_val_split.cpu().numpy(), y_val_split)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# È©óË≠â XGBoost\n",
    "xgb_val_preds = xgb_model.predict(X_val_split.cpu().numpy())\n",
    "print(\"XGBoost (GPU) Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val_split, xgb_val_preds))\n",
    "print(\"\\nXGBoost (GPU) Classification Report:\")\n",
    "print(classification_report(y_val_split, xgb_val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61c8e8b6-7ec6-4b1d-9e62-96275755ae31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 229051\n",
      "[LightGBM] [Info] Number of data points in the train set: 579672, number of used features: 1372\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3080, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 871 dense feature groups (482.06 MB) transferred to GPU in 0.163718 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -3.601124\n",
      "[LightGBM] [Info] Start training from score -1.767484\n",
      "[LightGBM] [Info] Start training from score -2.341639\n",
      "[LightGBM] [Info] Start training from score -3.125354\n",
      "[LightGBM] [Info] Start training from score -1.036238\n",
      "[LightGBM] [Info] Start training from score -2.018709\n",
      "[LightGBM] [Info] Start training from score -3.418971\n",
      "[LightGBM] [Info] Start training from score -1.955499\n",
      "LightGBM Confusion Matrix:\n",
      "[[  581   189   714    51  1406   962     7    45]\n",
      " [   52 12287   865   124  9275  1481    13   650]\n",
      " [  151   515  4865   166  4474  3595    24   147]\n",
      " [   22   477   467  1559  2874   889    10    67]\n",
      " [   75  3139  1329   252 42817  2426    28  1350]\n",
      " [  175   812  2724   210  7084  8046    27   171]\n",
      " [   18   233   619    59  2180   977   600    59]\n",
      " [   34  1958   662    70 12307  1056     8  4410]]\n",
      "\n",
      "LightGBM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.15      0.23      3955\n",
      "           1       0.63      0.50      0.55     24747\n",
      "           2       0.40      0.35      0.37     13937\n",
      "           3       0.63      0.24      0.35      6365\n",
      "           4       0.52      0.83      0.64     51416\n",
      "           5       0.41      0.42      0.42     19249\n",
      "           6       0.84      0.13      0.22      4745\n",
      "           7       0.64      0.22      0.32     20505\n",
      "\n",
      "    accuracy                           0.52    144919\n",
      "   macro avg       0.57      0.35      0.39    144919\n",
      "weighted avg       0.54      0.52      0.49    144919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective='multiclass', \n",
    "    num_class=len(label_encoder.classes_), \n",
    "    boosting_type='gbdt', \n",
    "    max_depth=-1, \n",
    "    learning_rate=0.1, \n",
    "    n_estimators=100,\n",
    "    device='gpu',              # ÂïüÁî® GPU\n",
    "    gpu_device_id=0            # ÊåáÂÆö GPU Âç° IDÔºåÈÄöÂ∏∏ÊòØ 0\n",
    ")\n",
    "\n",
    "# Ë®ìÁ∑¥ LightGBM\n",
    "lgb_model.fit(X_train_split.cpu().numpy(), y_train_split)\n",
    "\n",
    "# È©óË≠â LightGBM\n",
    "lgb_val_preds = lgb_model.predict(X_val_split.cpu().numpy())\n",
    "print(\"LightGBM Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val_split, lgb_val_preds))\n",
    "print(\"\\nLightGBM Classification Report:\")\n",
    "print(classification_report(y_val_split, lgb_val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9279ab19-a1cf-46c5-9080-9cfec5d9f2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t1070\\AppData\\Local\\Temp\\ipykernel_23660\\4204155006.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_train_split, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Loss: 0.6686749887300268\n",
      "Epoch 2/5, Average Loss: 0.6368004484660975\n",
      "Epoch 3/5, Average Loss: 0.6261931888325222\n",
      "Epoch 4/5, Average Loss: 0.6191142202106392\n",
      "Epoch 5/5, Average Loss: 0.613716012470603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t1070\\AppData\\Local\\Temp\\ipykernel_23660\\4204155006.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_val_split, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Confusion Matrix:\n",
      "[[  582   182   951    56  1278   820     5    81]\n",
      " [   33 13144  1038   193  8228  1066    21  1024]\n",
      " [  110   485  6209   146  3951  2672    26   338]\n",
      " [   10   424   666  2018  2442   647    10   148]\n",
      " [   43  3166  1836   355 41639  1865    24  2488]\n",
      " [  152   782  3727   223  6182  7723    27   433]\n",
      " [   17   236   905    66  2042   708   634   137]\n",
      " [   15  1934   822   111 10741   796    14  6072]]\n",
      "\n",
      "LSTM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.15      0.24      3955\n",
      "           1       0.65      0.53      0.58     24747\n",
      "           2       0.38      0.45      0.41     13937\n",
      "           3       0.64      0.32      0.42      6365\n",
      "           4       0.54      0.81      0.65     51416\n",
      "           5       0.47      0.40      0.43     19249\n",
      "           6       0.83      0.13      0.23      4745\n",
      "           7       0.57      0.30      0.39     20505\n",
      "\n",
      "    accuracy                           0.54    144919\n",
      "   macro avg       0.59      0.39      0.42    144919\n",
      "weighted avg       0.56      0.54      0.52    144919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Ensure x is 3D: [batch_size, sequence_length, features]\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)  # Add sequence dimension if missing\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = self.attention(lstm_out).squeeze(-1)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # Weighted sum of LSTM outputs\n",
    "        attended_output = torch.sum(attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n",
    "        \n",
    "        return self.fc(attended_output)\n",
    "\n",
    "# 2. LSTM+Attention Ê®°Âûã\n",
    "# ÊßãÂª∫ DataLoader\n",
    "batch_size = 16\n",
    "accumulation_steps = 2\n",
    "num_epochs = 5\n",
    "\n",
    "# Create dataset and loader\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train_split, dtype=torch.float32), \n",
    "    torch.tensor(y_train_split, dtype=torch.long)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "lstm_attention_model = LSTMWithAttention(\n",
    "    input_dim=X_train_split.shape[1], \n",
    "    hidden_dim=64,  # Reduced hidden dimension \n",
    "    output_dim=len(label_encoder.classes_)\n",
    ").to('cuda')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm_attention_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_attention_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for i, (batch_features, batch_labels) in enumerate(train_loader):\n",
    "        batch_features = batch_features.to('cuda')\n",
    "        batch_labels = batch_labels.to('cuda')\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = lstm_attention_model(batch_features)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, batch_labels) / accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Validation loop\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(X_val_split, dtype=torch.float32), \n",
    "    torch.tensor(y_val_split, dtype=torch.long)\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "lstm_attention_model.eval()\n",
    "val_preds = []\n",
    "val_true = []\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in val_loader:\n",
    "        batch_features = batch_features.to('cuda')\n",
    "        outputs = lstm_attention_model(batch_features)\n",
    "        batch_preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        val_preds.extend(batch_preds)\n",
    "        val_true.extend(batch_labels.numpy())\n",
    "\n",
    "print(\"LSTM Confusion Matrix:\")\n",
    "print(confusion_matrix(val_true, val_preds))\n",
    "print(\"\\nLSTM Classification Report:\")\n",
    "print(classification_report(val_true, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb3fa6e9-ecb0-4a64-ae44-b18eeb5b5670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major Voting Confusion Matrix:\n",
      "[[  574   195   770    40  1468   875     1    32]\n",
      " [   39 12332   919    85  9615  1267     3   487]\n",
      " [  123   522  5228   132  4610  3207     7   108]\n",
      " [   19   468   539  1558  2974   751     2    54]\n",
      " [   61  2993  1473   197 43617  1989     8  1078]\n",
      " [  157   850  2931   170  7364  7653    10   114]\n",
      " [   18   219   678    51  2283   871   582    43]\n",
      " [   23  1933   725    53 12701   887     5  4178]]\n",
      "\n",
      "Major Voting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.15      0.23      3955\n",
      "           1       0.63      0.50      0.56     24747\n",
      "           2       0.39      0.38      0.38     13937\n",
      "           3       0.68      0.24      0.36      6365\n",
      "           4       0.52      0.85      0.64     51416\n",
      "           5       0.44      0.40      0.42     19249\n",
      "           6       0.94      0.12      0.22      4745\n",
      "           7       0.69      0.20      0.31     20505\n",
      "\n",
      "    accuracy                           0.52    144919\n",
      "   macro avg       0.61      0.35      0.39    144919\n",
      "weighted avg       0.56      0.52      0.49    144919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_val_preds = val_preds\n",
    "\n",
    "val_predictions = np.array([lstm_val_preds, xgb_val_preds, lgb_val_preds])\n",
    "major_voting_val_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=val_predictions)\n",
    "\n",
    "print(\"Major Voting Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val_split, major_voting_val_preds))\n",
    "print(\"\\nMajor Voting Classification Report:\")\n",
    "print(classification_report(y_val_split, major_voting_val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0297a9e-c2e0-4a74-8288-7f90eb916972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t1070\\AppData\\Local\\Temp\\ipykernel_23660\\1869440319.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_features_tensor = torch.tensor(test_features, dtype=torch.float32).to('cuda')\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Ê∏¨Ë©¶ÈõÜÈ†êÊ∏¨\n",
    "xgb_test_preds = xgb_model.predict(test_features.cpu().numpy())\n",
    "\n",
    "# Ê∏¨Ë©¶ÈõÜÈ†êÊ∏¨\n",
    "lgb_test_preds = lgb_model.predict(test_features.cpu().numpy())\n",
    "\n",
    "test_features_tensor = torch.tensor(test_features, dtype=torch.float32).to('cuda')\n",
    "\n",
    "# Predict on test set\n",
    "lstm_attention_model.eval()\n",
    "with torch.no_grad():\n",
    "    lstm_test_preds = torch.argmax(lstm_attention_model(test_features_tensor), dim=1).cpu().numpy()\n",
    "\n",
    "# Major Voting for Test\n",
    "# lstm_test_preds = y_test_pred  # LSTM Ê∏¨Ë©¶È†êÊ∏¨\n",
    "test_predictions = np.array([lstm_test_preds, xgb_test_preds, lgb_test_preds])\n",
    "major_voting_test_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "034f7355-4b93-4232-a93a-f321d10af720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform predictions\n",
    "test_df['emotion'] = label_encoder.inverse_transform(major_voting_test_preds)\n",
    "\n",
    "# Output submission\n",
    "submission = test_df[['tweet_id', 'emotion']]\n",
    "submission.to_csv('major_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
