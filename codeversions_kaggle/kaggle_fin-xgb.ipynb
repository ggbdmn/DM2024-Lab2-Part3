{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce48bf9b-b1ed-49e9-984c-3202ae81c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from contractions import fix\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e28ae8-6bd0-4e6c-8c4b-826e2003b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('tweet/tweets_DM.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line)) \n",
    "f.close()\n",
    "\n",
    "emotion = pd.read_csv('tweet/emotion.csv')\n",
    "data_identification = pd.read_csv('tweet/data_identification.csv')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "_source = df['_source'].apply(lambda x: x['tweet'])\n",
    "df = pd.DataFrame({\n",
    "    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n",
    "    'hashtags': _source.apply(lambda x: x['hashtags']),\n",
    "    'text': _source.apply(lambda x: x['text']),\n",
    "})\n",
    "df = df.merge(data_identification, on='tweet_id', how='left')\n",
    "\n",
    "train_data = df[df['identification'] == 'train']\n",
    "test_data = df[df['identification'] == 'test']\n",
    "\n",
    "train_data = train_data.merge(emotion, on='tweet_id', how='left')\n",
    "train_data.drop_duplicates(subset=['text'], keep=False, inplace=True)\n",
    "\n",
    "train_data_sample = train_data.sample(frac=0.5, random_state=42)\n",
    "train_data_sample.to_pickle(\"train_dsample.pkl\")\n",
    "train_df = pd.read_pickle(\"train_dsample.pkl\")\n",
    "\n",
    "test_data.to_pickle(\"test_d.pkl\")\n",
    "test_df = pd.read_pickle(\"test_d.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f32ee6-46b1-4975-a8db-e3ff29e1b2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b287f7f5d8e467cb2ae4fc3287749cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/724591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177b90b6909d4e419760fcd517b99d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/411972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import wordnet\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 表情符號替換詞典\n",
    "emoji_dict = {\n",
    "    '😂': '[joy]', '❤️': '[love]', '😍': '[adoration]', '😭': '[cry]',\n",
    "    '❤': '[care]', '😊': '[happy]', '🙏': '[pray]', '😘': '[kiss]',\n",
    "    '💕': '[love_each_other]', '🔥': '[fire]', '😩': '[weary]',\n",
    "    '🤔': '[think]', '💯': '[perfect]', '💙': '[loyalty]',\n",
    "    '🙄': '[annoyed]', '😁': '[happy]', '🙌': '[celebrate]',\n",
    "    '🙏🏾': '[pray]', '👍': '[approve]', '🙏🏽': '[pray]'\n",
    "}\n",
    "\n",
    "# Define a dictionary for common Twitter abbreviations/slangs\n",
    "slang_dict = {\n",
    "    \"lol\": \"laugh out_loud\",\n",
    "    \"u\": \"you\",\n",
    "    \"idk\": \"I do not know\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"lmao\": \"laugh my_ass_off\",\n",
    "    \"lmfao\": \"laugh my_ass_off\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"brb\": \"be right back\"\n",
    "    # Add more as needed\n",
    "}\n",
    "\n",
    "# 預處理函數\n",
    "def preprocess_text(text):\n",
    "    # 替換 emoji\n",
    "    for emj, keyword in emoji_dict.items():\n",
    "        text = text.replace(emj, keyword)\n",
    "    text = emoji.replace_emoji(text, replace='')  # 移除其他 emoji\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # 移除網址\n",
    "    text = re.sub(r'RT[\\s]+', '', text)  # Remove RT\n",
    "    text = text.replace('<LH>', '')\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)  # 移除 @user 和 hashtags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", '', text)  # 移除特殊字元\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s!?]', '', text)\n",
    "    text = re.sub(r'not\\s+(\\w+)', r'not_\\1', text)\n",
    "    \n",
    "    wds = text.split()\n",
    "    tweet = \" \".join([slang_dict[wd.lower()] if wd.lower() in slang_dict else wd for wd in wds])\n",
    "    \n",
    "    text = fix(text)\n",
    "    #text = str(TextBlob(text).correct())\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = \" \".join([lemmatizer.lemmatize(wd) for wd in text.split()])\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([word for word in words if word not in stop_words])\n",
    "\n",
    "# 清理訓練與測試資料\n",
    "train_df['clean_text'] = train_df['text'].swifter.apply(preprocess_text)\n",
    "test_df['clean_text'] = test_df['text'].swifter.apply(preprocess_text)\n",
    "\n",
    "# 打亂訓練資料\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4435f750-620b-4b98-b304-809e3b4608b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 將 hashtags 組合成字串\n",
    "train_df['hashtags'] = train_df['hashtags'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "test_df['hashtags'] = test_df['hashtags'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "\n",
    "# 訓練 TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "tfidf_train = tfidf.fit_transform(train_df['hashtags'])\n",
    "tfidf_test = tfidf.transform(test_df['hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054ba0db-2d16-4490-9b8b-b471235cfcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# 將每段文字轉為詞列表\n",
    "train_sentences = train_df['clean_text'].apply(lambda x: x.split()).tolist()\n",
    "test_sentences = test_df['clean_text'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# 訓練 Word2Vec 模型\n",
    "w2v_model = Word2Vec(sentences=train_sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# 將文字轉換為向量平均值\n",
    "def sentence_to_vector(sentence, model):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "train_text_vectors = np.array([sentence_to_vector(sent, w2v_model) for sent in train_sentences])\n",
    "test_text_vectors = np.array([sentence_to_vector(sent, w2v_model) for sent in test_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72fbbc3-4c11-4884-892c-e2ded24e592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# 將 TF-IDF 與 Word2Vec 合併\n",
    "X_train = hstack([tfidf_train, train_text_vectors])\n",
    "X_test = hstack([tfidf_test, test_text_vectors])\n",
    "\n",
    "# 訓練標籤\n",
    "y_train = train_df['emotion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d7c12dd-8411-4f7f-be40-8a59d27463c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 將文字標籤轉換為數值\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# 分割訓練與驗證資料\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# 訓練模型\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(label_encoder.classes_))\n",
    "xgb_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# 預測驗證集\n",
    "y_pred_val_encoded = xgb_model.predict(X_val_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90bbd4f5-4b16-4947-a04d-f886212ac8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  460   196   638    37  1737   769     4    38]\n",
      " [   32 11747   880   137 10413  1243    14   486]\n",
      " [   70   635  4319   152  5325  3074     9   120]\n",
      " [    6   455   431  1623  3175   653     7    56]\n",
      " [   32  3258  1455   305 42798  2354    28  1251]\n",
      " [   82   979  2585   204  8177  6952    17   198]\n",
      " [    8   235   599    53  2447   781   614    72]\n",
      " [   12  1950   811    78 12834  1042    14  3753]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.66      0.12      0.20      3879\n",
      "anticipation       0.60      0.47      0.53     24952\n",
      "     disgust       0.37      0.32      0.34     13704\n",
      "        fear       0.63      0.25      0.36      6406\n",
      "         joy       0.49      0.83      0.62     51481\n",
      "     sadness       0.41      0.36      0.39     19194\n",
      "    surprise       0.87      0.13      0.22      4809\n",
      "       trust       0.63      0.18      0.28     20494\n",
      "\n",
      "    accuracy                           0.50    144919\n",
      "   macro avg       0.58      0.33      0.37    144919\n",
      "weighted avg       0.53      0.50      0.46    144919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 將數值標籤轉回文字標籤\n",
    "y_pred_val = label_encoder.inverse_transform(y_pred_val_encoded)\n",
    "y_val_split_text = label_encoder.inverse_transform(y_val_split)\n",
    "\n",
    "# 評估模型\n",
    "print(confusion_matrix(y_val_split_text, y_pred_val))\n",
    "print(classification_report(y_val_split_text, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9158e6e2-5878-4afe-9677-e61089c837da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預測測試集\n",
    "y_test_pred_encoded = xgb_model.predict(X_test)\n",
    "test_df['emotion'] = label_encoder.inverse_transform(y_test_pred_encoded)\n",
    "\n",
    "# 輸出為 submission.csv\n",
    "submission = test_df[['tweet_id', 'emotion']]\n",
    "submission.to_csv('submission_fin1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7869f0-8b44-4057-aed1-dcc8d2afc5c7",
   "metadata": {},
   "source": [
    "1. 你認為使用xgboost是好方法嗎 還是有別的更好的辦法 code要做那些修改\n",
    "2. 你認為降維會提高最後的結果嗎 應該在哪裡降\n",
    "3. 你認為前處理還有什麼可以更加強的做法嗎 例如正則化之類的 或是其他 請提出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "774f8cee-32b9-4845-8b4a-e70f97acb0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x36fc6e</td>\n",
       "      <td>development future winner</td>\n",
       "      <td>Escaping pain is not the answer. Embracing pai...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>escaping pain not_the answer embracing pain al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x36f312</td>\n",
       "      <td>ignored</td>\n",
       "      <td>If I don't like you more then likely you've be...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>like likely disrespectful unpleasant soul plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1d7398</td>\n",
       "      <td>Silverdome</td>\n",
       "      <td>Two stadiums I've been two and photographed, i...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>two stadium two photographed imploded within w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x26d0d1</td>\n",
       "      <td>trans</td>\n",
       "      <td>The racial trans badge  #trans &lt;LH&gt; &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>racial trans badge trans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c580d</td>\n",
       "      <td>weddingdressfitting</td>\n",
       "      <td>Very special day with Luda and her mom ❤️Feeli...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>special day luda mom lovefeeling blessed weddi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724586</th>\n",
       "      <td>0x380c45</td>\n",
       "      <td>Power5at5</td>\n",
       "      <td>Hey @POWERATL @maddoxradio please play &lt;LH&gt; by...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>hey please play power5at5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724587</th>\n",
       "      <td>0x36c504</td>\n",
       "      <td></td>\n",
       "      <td>damn my foot healed when everyone is already i...</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "      <td>damn foot healed everyone already tj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724588</th>\n",
       "      <td>0x2e8018</td>\n",
       "      <td>ZENii skincareroutine moisturiser health healt...</td>\n",
       "      <td>@skinandbodyclin we're excited 🙌 #ZENii #skinc...</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>excited celebrate zenii skincareroutine moistu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724589</th>\n",
       "      <td>0x31e324</td>\n",
       "      <td>job</td>\n",
       "      <td>So excited! Just got a call that I have an int...</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "      <td>excited got call interview wednesday job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724590</th>\n",
       "      <td>0x23f502</td>\n",
       "      <td>BB19Finale</td>\n",
       "      <td>didn't know banging pots and pans all season c...</td>\n",
       "      <td>train</td>\n",
       "      <td>anger</td>\n",
       "      <td>know banging pot pan season could get vote win...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>724591 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweet_id                                           hashtags  \\\n",
       "0       0x36fc6e                          development future winner   \n",
       "1       0x36f312                                            ignored   \n",
       "2       0x1d7398                                         Silverdome   \n",
       "3       0x26d0d1                                              trans   \n",
       "4       0x2c580d                                weddingdressfitting   \n",
       "...          ...                                                ...   \n",
       "724586  0x380c45                                          Power5at5   \n",
       "724587  0x36c504                                                      \n",
       "724588  0x2e8018  ZENii skincareroutine moisturiser health healt...   \n",
       "724589  0x31e324                                                job   \n",
       "724590  0x23f502                                         BB19Finale   \n",
       "\n",
       "                                                     text identification  \\\n",
       "0       Escaping pain is not the answer. Embracing pai...          train   \n",
       "1       If I don't like you more then likely you've be...          train   \n",
       "2       Two stadiums I've been two and photographed, i...          train   \n",
       "3                The racial trans badge  #trans <LH> <LH>          train   \n",
       "4       Very special day with Luda and her mom ❤️Feeli...          train   \n",
       "...                                                   ...            ...   \n",
       "724586  Hey @POWERATL @maddoxradio please play <LH> by...          train   \n",
       "724587  damn my foot healed when everyone is already i...          train   \n",
       "724588  @skinandbodyclin we're excited 🙌 #ZENii #skinc...          train   \n",
       "724589  So excited! Just got a call that I have an int...          train   \n",
       "724590  didn't know banging pots and pans all season c...          train   \n",
       "\n",
       "        emotion                                         clean_text  \n",
       "0       sadness  escaping pain not_the answer embracing pain al...  \n",
       "1       sadness  like likely disrespectful unpleasant soul plea...  \n",
       "2       sadness  two stadium two photographed imploded within w...  \n",
       "3         trust                           racial trans badge trans  \n",
       "4           joy  special day luda mom lovefeeling blessed weddi...  \n",
       "...         ...                                                ...  \n",
       "724586  sadness                          hey please play power5at5  \n",
       "724587  disgust               damn foot healed everyone already tj  \n",
       "724588    trust  excited celebrate zenii skincareroutine moistu...  \n",
       "724589     fear           excited got call interview wednesday job  \n",
       "724590    anger  know banging pot pan season could get vote win...  \n",
       "\n",
       "[724591 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
