{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce48bf9b-b1ed-49e9-984c-3202ae81c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from contractions import fix\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e28ae8-6bd0-4e6c-8c4b-826e2003b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('tweet/tweets_DM.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line)) \n",
    "f.close()\n",
    "\n",
    "emotion = pd.read_csv('tweet/emotion.csv')\n",
    "data_identification = pd.read_csv('tweet/data_identification.csv')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "_source = df['_source'].apply(lambda x: x['tweet'])\n",
    "df = pd.DataFrame({\n",
    "    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n",
    "    'hashtags': _source.apply(lambda x: x['hashtags']),\n",
    "    'text': _source.apply(lambda x: x['text']),\n",
    "})\n",
    "df = df.merge(data_identification, on='tweet_id', how='left')\n",
    "\n",
    "train_data = df[df['identification'] == 'train']\n",
    "test_data = df[df['identification'] == 'test']\n",
    "\n",
    "train_data = train_data.merge(emotion, on='tweet_id', how='left')\n",
    "train_data.drop_duplicates(subset=['text'], keep=False, inplace=True)\n",
    "\n",
    "train_data_sample = train_data.sample(frac=0.5, random_state=42)\n",
    "train_data_sample.to_pickle(\"train_dsample.pkl\")\n",
    "train_df = pd.read_pickle(\"train_dsample.pkl\")\n",
    "\n",
    "test_data.to_pickle(\"test_d.pkl\")\n",
    "test_df = pd.read_pickle(\"test_d.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f32ee6-46b1-4975-a8db-e3ff29e1b2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b287f7f5d8e467cb2ae4fc3287749cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/724591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177b90b6909d4e419760fcd517b99d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/411972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import wordnet\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# è¡¨æƒ…ç¬¦è™Ÿæ›¿æ›è©å…¸\n",
    "emoji_dict = {\n",
    "    'ğŸ˜‚': '[joy]', 'â¤ï¸': '[love]', 'ğŸ˜': '[adoration]', 'ğŸ˜­': '[cry]',\n",
    "    'â¤': '[care]', 'ğŸ˜Š': '[happy]', 'ğŸ™': '[pray]', 'ğŸ˜˜': '[kiss]',\n",
    "    'ğŸ’•': '[love_each_other]', 'ğŸ”¥': '[fire]', 'ğŸ˜©': '[weary]',\n",
    "    'ğŸ¤”': '[think]', 'ğŸ’¯': '[perfect]', 'ğŸ’™': '[loyalty]',\n",
    "    'ğŸ™„': '[annoyed]', 'ğŸ˜': '[happy]', 'ğŸ™Œ': '[celebrate]',\n",
    "    'ğŸ™ğŸ¾': '[pray]', 'ğŸ‘': '[approve]', 'ğŸ™ğŸ½': '[pray]'\n",
    "}\n",
    "\n",
    "# Define a dictionary for common Twitter abbreviations/slangs\n",
    "slang_dict = {\n",
    "    \"lol\": \"laugh out_loud\",\n",
    "    \"u\": \"you\",\n",
    "    \"idk\": \"I do not know\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"lmao\": \"laugh my_ass_off\",\n",
    "    \"lmfao\": \"laugh my_ass_off\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"brb\": \"be right back\"\n",
    "    # Add more as needed\n",
    "}\n",
    "\n",
    "# é è™•ç†å‡½æ•¸\n",
    "def preprocess_text(text):\n",
    "    # æ›¿æ› emoji\n",
    "    for emj, keyword in emoji_dict.items():\n",
    "        text = text.replace(emj, keyword)\n",
    "    text = emoji.replace_emoji(text, replace='')  # ç§»é™¤å…¶ä»– emoji\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # ç§»é™¤ç¶²å€\n",
    "    text = re.sub(r'RT[\\s]+', '', text)  # Remove RT\n",
    "    text = text.replace('<LH>', '')\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)  # ç§»é™¤ @user å’Œ hashtags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", '', text)  # ç§»é™¤ç‰¹æ®Šå­—å…ƒ\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s!?]', '', text)\n",
    "    text = re.sub(r'not\\s+(\\w+)', r'not_\\1', text)\n",
    "    \n",
    "    wds = text.split()\n",
    "    tweet = \" \".join([slang_dict[wd.lower()] if wd.lower() in slang_dict else wd for wd in wds])\n",
    "    \n",
    "    text = fix(text)\n",
    "    #text = str(TextBlob(text).correct())\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = \" \".join([lemmatizer.lemmatize(wd) for wd in text.split()])\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([word for word in words if word not in stop_words])\n",
    "\n",
    "# æ¸…ç†è¨“ç·´èˆ‡æ¸¬è©¦è³‡æ–™\n",
    "train_df['clean_text'] = train_df['text'].swifter.apply(preprocess_text)\n",
    "test_df['clean_text'] = test_df['text'].swifter.apply(preprocess_text)\n",
    "\n",
    "# æ‰“äº‚è¨“ç·´è³‡æ–™\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4435f750-620b-4b98-b304-809e3b4608b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# å°‡ hashtags çµ„åˆæˆå­—ä¸²\n",
    "train_df['hashtags'] = train_df['hashtags'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "test_df['hashtags'] = test_df['hashtags'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "\n",
    "# è¨“ç·´ TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "tfidf_train = tfidf.fit_transform(train_df['hashtags'])\n",
    "tfidf_test = tfidf.transform(test_df['hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054ba0db-2d16-4490-9b8b-b471235cfcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# å°‡æ¯æ®µæ–‡å­—è½‰ç‚ºè©åˆ—è¡¨\n",
    "train_sentences = train_df['clean_text'].apply(lambda x: x.split()).tolist()\n",
    "test_sentences = test_df['clean_text'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# è¨“ç·´ Word2Vec æ¨¡å‹\n",
    "w2v_model = Word2Vec(sentences=train_sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# å°‡æ–‡å­—è½‰æ›ç‚ºå‘é‡å¹³å‡å€¼\n",
    "def sentence_to_vector(sentence, model):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "train_text_vectors = np.array([sentence_to_vector(sent, w2v_model) for sent in train_sentences])\n",
    "test_text_vectors = np.array([sentence_to_vector(sent, w2v_model) for sent in test_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72fbbc3-4c11-4884-892c-e2ded24e592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# å°‡ TF-IDF èˆ‡ Word2Vec åˆä½µ\n",
    "X_train = hstack([tfidf_train, train_text_vectors])\n",
    "X_test = hstack([tfidf_test, test_text_vectors])\n",
    "\n",
    "# è¨“ç·´æ¨™ç±¤\n",
    "y_train = train_df['emotion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d7c12dd-8411-4f7f-be40-8a59d27463c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# å°‡æ–‡å­—æ¨™ç±¤è½‰æ›ç‚ºæ•¸å€¼\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# åˆ†å‰²è¨“ç·´èˆ‡é©—è­‰è³‡æ–™\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# è¨“ç·´æ¨¡å‹\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(label_encoder.classes_))\n",
    "xgb_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# é æ¸¬é©—è­‰é›†\n",
    "y_pred_val_encoded = xgb_model.predict(X_val_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90bbd4f5-4b16-4947-a04d-f886212ac8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  460   196   638    37  1737   769     4    38]\n",
      " [   32 11747   880   137 10413  1243    14   486]\n",
      " [   70   635  4319   152  5325  3074     9   120]\n",
      " [    6   455   431  1623  3175   653     7    56]\n",
      " [   32  3258  1455   305 42798  2354    28  1251]\n",
      " [   82   979  2585   204  8177  6952    17   198]\n",
      " [    8   235   599    53  2447   781   614    72]\n",
      " [   12  1950   811    78 12834  1042    14  3753]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.66      0.12      0.20      3879\n",
      "anticipation       0.60      0.47      0.53     24952\n",
      "     disgust       0.37      0.32      0.34     13704\n",
      "        fear       0.63      0.25      0.36      6406\n",
      "         joy       0.49      0.83      0.62     51481\n",
      "     sadness       0.41      0.36      0.39     19194\n",
      "    surprise       0.87      0.13      0.22      4809\n",
      "       trust       0.63      0.18      0.28     20494\n",
      "\n",
      "    accuracy                           0.50    144919\n",
      "   macro avg       0.58      0.33      0.37    144919\n",
      "weighted avg       0.53      0.50      0.46    144919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# å°‡æ•¸å€¼æ¨™ç±¤è½‰å›æ–‡å­—æ¨™ç±¤\n",
    "y_pred_val = label_encoder.inverse_transform(y_pred_val_encoded)\n",
    "y_val_split_text = label_encoder.inverse_transform(y_val_split)\n",
    "\n",
    "# è©•ä¼°æ¨¡å‹\n",
    "print(confusion_matrix(y_val_split_text, y_pred_val))\n",
    "print(classification_report(y_val_split_text, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9158e6e2-5878-4afe-9677-e61089c837da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é æ¸¬æ¸¬è©¦é›†\n",
    "y_test_pred_encoded = xgb_model.predict(X_test)\n",
    "test_df['emotion'] = label_encoder.inverse_transform(y_test_pred_encoded)\n",
    "\n",
    "# è¼¸å‡ºç‚º submission.csv\n",
    "submission = test_df[['tweet_id', 'emotion']]\n",
    "submission.to_csv('submission_fin1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7869f0-8b44-4057-aed1-dcc8d2afc5c7",
   "metadata": {},
   "source": [
    "1. ä½ èªç‚ºä½¿ç”¨xgboostæ˜¯å¥½æ–¹æ³•å— é‚„æ˜¯æœ‰åˆ¥çš„æ›´å¥½çš„è¾¦æ³• codeè¦åšé‚£äº›ä¿®æ”¹\n",
    "2. ä½ èªç‚ºé™ç¶­æœƒæé«˜æœ€å¾Œçš„çµæœå— æ‡‰è©²åœ¨å“ªè£¡é™\n",
    "3. ä½ èªç‚ºå‰è™•ç†é‚„æœ‰ä»€éº¼å¯ä»¥æ›´åŠ å¼·çš„åšæ³•å— ä¾‹å¦‚æ­£å‰‡åŒ–ä¹‹é¡çš„ æˆ–æ˜¯å…¶ä»– è«‹æå‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "774f8cee-32b9-4845-8b4a-e70f97acb0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x36fc6e</td>\n",
       "      <td>development future winner</td>\n",
       "      <td>Escaping pain is not the answer. Embracing pai...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>escaping pain not_the answer embracing pain al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x36f312</td>\n",
       "      <td>ignored</td>\n",
       "      <td>If I don't like you more then likely you've be...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>like likely disrespectful unpleasant soul plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1d7398</td>\n",
       "      <td>Silverdome</td>\n",
       "      <td>Two stadiums I've been two and photographed, i...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>two stadium two photographed imploded within w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x26d0d1</td>\n",
       "      <td>trans</td>\n",
       "      <td>The racial trans badge  #trans &lt;LH&gt; &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>racial trans badge trans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c580d</td>\n",
       "      <td>weddingdressfitting</td>\n",
       "      <td>Very special day with Luda and her mom â¤ï¸Feeli...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>special day luda mom lovefeeling blessed weddi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724586</th>\n",
       "      <td>0x380c45</td>\n",
       "      <td>Power5at5</td>\n",
       "      <td>Hey @POWERATL @maddoxradio please play &lt;LH&gt; by...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>hey please play power5at5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724587</th>\n",
       "      <td>0x36c504</td>\n",
       "      <td></td>\n",
       "      <td>damn my foot healed when everyone is already i...</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "      <td>damn foot healed everyone already tj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724588</th>\n",
       "      <td>0x2e8018</td>\n",
       "      <td>ZENii skincareroutine moisturiser health healt...</td>\n",
       "      <td>@skinandbodyclin we're excited ğŸ™Œ #ZENii #skinc...</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>excited celebrate zenii skincareroutine moistu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724589</th>\n",
       "      <td>0x31e324</td>\n",
       "      <td>job</td>\n",
       "      <td>So excited! Just got a call that I have an int...</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "      <td>excited got call interview wednesday job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724590</th>\n",
       "      <td>0x23f502</td>\n",
       "      <td>BB19Finale</td>\n",
       "      <td>didn't know banging pots and pans all season c...</td>\n",
       "      <td>train</td>\n",
       "      <td>anger</td>\n",
       "      <td>know banging pot pan season could get vote win...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>724591 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweet_id                                           hashtags  \\\n",
       "0       0x36fc6e                          development future winner   \n",
       "1       0x36f312                                            ignored   \n",
       "2       0x1d7398                                         Silverdome   \n",
       "3       0x26d0d1                                              trans   \n",
       "4       0x2c580d                                weddingdressfitting   \n",
       "...          ...                                                ...   \n",
       "724586  0x380c45                                          Power5at5   \n",
       "724587  0x36c504                                                      \n",
       "724588  0x2e8018  ZENii skincareroutine moisturiser health healt...   \n",
       "724589  0x31e324                                                job   \n",
       "724590  0x23f502                                         BB19Finale   \n",
       "\n",
       "                                                     text identification  \\\n",
       "0       Escaping pain is not the answer. Embracing pai...          train   \n",
       "1       If I don't like you more then likely you've be...          train   \n",
       "2       Two stadiums I've been two and photographed, i...          train   \n",
       "3                The racial trans badge  #trans <LH> <LH>          train   \n",
       "4       Very special day with Luda and her mom â¤ï¸Feeli...          train   \n",
       "...                                                   ...            ...   \n",
       "724586  Hey @POWERATL @maddoxradio please play <LH> by...          train   \n",
       "724587  damn my foot healed when everyone is already i...          train   \n",
       "724588  @skinandbodyclin we're excited ğŸ™Œ #ZENii #skinc...          train   \n",
       "724589  So excited! Just got a call that I have an int...          train   \n",
       "724590  didn't know banging pots and pans all season c...          train   \n",
       "\n",
       "        emotion                                         clean_text  \n",
       "0       sadness  escaping pain not_the answer embracing pain al...  \n",
       "1       sadness  like likely disrespectful unpleasant soul plea...  \n",
       "2       sadness  two stadium two photographed imploded within w...  \n",
       "3         trust                           racial trans badge trans  \n",
       "4           joy  special day luda mom lovefeeling blessed weddi...  \n",
       "...         ...                                                ...  \n",
       "724586  sadness                          hey please play power5at5  \n",
       "724587  disgust               damn foot healed everyone already tj  \n",
       "724588    trust  excited celebrate zenii skincareroutine moistu...  \n",
       "724589     fear           excited got call interview wednesday job  \n",
       "724590    anger  know banging pot pan season could get vote win...  \n",
       "\n",
       "[724591 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
